# predictive-model
## Цель

Разработать прогностическую модель для набора данных диабетических обследований с использованием логистической регрессии и метода максимального правдоподобия. Коэффициенты логистической регрессии необходимо найти с помощью метода градиентного спуска, реализованного вручную. Также применить отбор признаков на основе корреляции и построить новую модель с уменьшенным числом признаков.

### Загрузка и предобработка данных:

Мы начали с загрузки данных из файла diabetes.txt, где каждая строка представляет собой набор из восьми признаков и целевую переменную. Мы присвоили признакам соответствующие имена и разделили данные на признаки (X) и целевую переменную (y).

### Нормализация данных:

Для обеспечения лучшей сходимости градиентного спуска мы применили нормализацию к признакам с помощью StandardScaler из библиотеки scikit-learn.
Реализация логистической регрессии с градиентным спуском:
Далее мы реализовали класс LogisticRegressionGD, который выполняет логистическую регрессию с использованием градиентного спуска. Внутри этого класса мы определили методы для вычисления сигмоиды (используемой для преобразования линейной комбинации в вероятность), вычисления функции потерь и обновления параметров модели с использованием градиентного спуска.

### Обучение и оценка модели:

Мы обучили модель логистической регрессии на обучающей выборке и оценили её точность на тестовой выборке. Также выводили различные метрики, такие как текущие потери, максимальный градиент, bias, максимальное значение весов и точность, чтобы отслеживать процесс обучения модели.

### Отбор признаков на основе корреляции:

Мы провели анализ корреляции признаков с целевой переменной и отобрали два наименее коррелированных признака для удаления.

### Построение новой модели и оценка её точности:

После удаления наименее коррелированных признаков мы построили новую модель логистической регрессии на обучающей выборке и оценили её точность на тестовой выборке.


Исходная модель:
•	Итерация 0:
•	Текущие потери: 0.6931471805599454
•	Максимальный градиент: 0.21768265263467804
•	Bias: -0.0015309446254071664
•	Максимальное значение весов: 0.0021768265263467806
•	Точность: 75.41%
•	Итерация 1000:
•	Текущие потери: 0.48017512394296047
•	Максимальный градиент: 0.028981735043739694
•	Bias: -0.6383206987404562
•	Максимальное значение весов: 0.7622169355838971
•	Точность: 76.87%
•	Итерация 2000:
•	Текущие потери: 0.47010460869791126
•	Максимальный градиент: 0.010931879838016552
•	Bias: -0.7785971390937793
•	Максимальное значение весов: 0.9424437956481069
•	Точность: 76.55%
•	Итерация 3000:
•	Текущие потери: 0.46835775351586817
•	Максимальный градиент: 0.00497381447978499
•	Bias: -0.8256054199347871
•	Максимальное значение весов: 1.0174165604980057
•	Точность: 76.71%
•	Итерация 10000:
•	Текущие потери: 0.46783513069616073
•	Максимальный градиент: 5.21862127539087e-05
•	Bias: -0.8610250611480686
•	Максимальное значение весов: 1.0889065449860058
•	Точность: 77.04%
Модель с отобранными признаками:
•	Итерация 0:
•	Текущие потери: 0.6931471805599454
•	Максимальный градиент: 0.21768265263467804
•	Bias: -0.0015309446254071664
•	Максимальное значение весов: 0.0021768265263467806
•	Точность: 75.41%
•	Итерация 1000:
•	Текущие потери: 0.4821795740389228
•	Максимальный градиент: 0.028222713310156005
•	Bias: -0.6406925916265397
•	Максимальное значение весов: 0.7573431529304464
•	Точность: 77.04%
•	Итерация 2000:
•	Текущие потери: 0.4736459360135491
•	Максимальный градиент: 0.010549374323451472
•	Bias: -0.783711913088337
•	Максимальное значение весов: 0.9321692356403968
•	Точность: 76.71%
•	Итерация 10000:
•	Текущие потери: 0.47204550350770613
•	Максимальный градиент: 2.514750640463057e-05
•	Bias: -0.8611469895632582
•	Максимальное значение весов: 1.0648546118654771
•	Точность: 77.04%

## Выводы:
•	Обе модели показывают устойчивое снижение потерь с увеличением числа итераций градиентного спуска.
•	Изначально, точность модели составляла около 75.41%.
•	После 10,000 итераций точность увеличилась до 77.04% как для исходной, так и для модели с отобранными признаками.
•	Таким образом, модель с отобранными признаками не привела к значимому улучшению точности в сравнении с полной моделью, что может свидетельствовать о том, что отобранные признаки несут меньше информации о целевой переменной.
•	Для дальнейшего улучшения точности модели может потребоваться более тщательный отбор признаков или использование более сложных методов обработки данных.
